{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 的士调度 Taxi-v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 环境使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观察空间 = Discrete(500)\n",
      "动作空间 = Discrete(6)\n",
      "状态数量 = 500\n",
      "动作数量 = 6\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.seed(0)\n",
    "print('观察空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('状态数量 = {}'.format(env.observation_space.n))\n",
    "print('动作数量 = {}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 1 2\n",
      "的士位置 = (0, 1)\n",
      "乘客位置 = (0, 4)\n",
      "目标位置 = (4, 0)\n",
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "taxirow, taxicol, passloc, destidx = env.unwrapped.decode(state)\n",
    "print(taxirow, taxicol, passloc, destidx)\n",
    "print('的士位置 = {}'.format((taxirow, taxicol)))\n",
    "print('乘客位置 = {}'.format(env.unwrapped.locs[passloc]))\n",
    "print('目标位置 = {}'.format(env.unwrapped.locs[destidx]))\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, -1, False, {'prob': 1.0})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    def __init__(self, env, gamma=0.9, learning_rate=0.2, epsilon=.01):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.action_n = env.action_space.n\n",
    "        self.q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        \n",
    "    def decide(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            action = self.q[state].argmax()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done, next_action):\n",
    "        u = reward + self.gamma * \\\n",
    "                self.q[next_state, next_action] * (1. - done)\n",
    "        td_error = u - self.q[state, action]\n",
    "        self.q[state, action] += self.learning_rate * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_sarsa(env, agent, train=False, render=False):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    action = agent.decide(observation)\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        next_action = agent.decide(next_observation) # 终止状态时此步无意义\n",
    "        if train:\n",
    "            agent.learn(observation, action, reward, next_observation,\n",
    "                    done, next_action)\n",
    "        if done:\n",
    "            break\n",
    "        observation, action = next_observation, next_action\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均回合奖励 = 834 / 100 = 8.34\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = SARSAAgent(env)\n",
    "\n",
    "# 训练\n",
    "episodes = 3000\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_sarsa(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# 测试\n",
    "agent.epsilon = 0. # 取消探索\n",
    "\n",
    "episode_rewards = [play_sarsa(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),\n",
    "        len(episode_rewards), np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示最优价值估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.906945</td>\n",
       "      <td>-3.926261</td>\n",
       "      <td>-3.354537</td>\n",
       "      <td>-3.844973</td>\n",
       "      <td>0.169093</td>\n",
       "      <td>-7.107725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.032215</td>\n",
       "      <td>-0.451203</td>\n",
       "      <td>-2.069749</td>\n",
       "      <td>-2.175503</td>\n",
       "      <td>7.713113</td>\n",
       "      <td>-3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.152140</td>\n",
       "      <td>-3.810306</td>\n",
       "      <td>-3.137979</td>\n",
       "      <td>-3.851483</td>\n",
       "      <td>2.509615</td>\n",
       "      <td>-7.105844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.791234</td>\n",
       "      <td>-5.933590</td>\n",
       "      <td>-5.896414</td>\n",
       "      <td>-5.953635</td>\n",
       "      <td>-7.072424</td>\n",
       "      <td>-7.107966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>-2.843914</td>\n",
       "      <td>-2.876611</td>\n",
       "      <td>-2.881306</td>\n",
       "      <td>-2.877551</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>-1.680592</td>\n",
       "      <td>-1.225120</td>\n",
       "      <td>-1.385155</td>\n",
       "      <td>-1.430747</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>-2.917878</td>\n",
       "      <td>-2.961647</td>\n",
       "      <td>-3.112380</td>\n",
       "      <td>-2.918050</td>\n",
       "      <td>-5.008434</td>\n",
       "      <td>-3.636000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>-0.707040</td>\n",
       "      <td>-0.424800</td>\n",
       "      <td>-0.707040</td>\n",
       "      <td>8.373279</td>\n",
       "      <td>-3.600000</td>\n",
       "      <td>-3.636000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5\n",
       "0    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "1   -3.906945 -3.926261 -3.354537 -3.844973  0.169093 -7.107725\n",
       "2   -2.032215 -0.451203 -2.069749 -2.175503  7.713113 -3.636000\n",
       "3   -3.152140 -3.810306 -3.137979 -3.851483  2.509615 -7.105844\n",
       "4   -5.791234 -5.933590 -5.896414 -5.953635 -7.072424 -7.107966\n",
       "..        ...       ...       ...       ...       ...       ...\n",
       "495  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000\n",
       "496 -2.843914 -2.876611 -2.881306 -2.877551 -3.600000 -3.636000\n",
       "497 -1.680592 -1.225120 -1.385155 -1.430747 -3.600000 -3.636000\n",
       "498 -2.917878 -2.961647 -3.112380 -2.918050 -5.008434 -3.636000\n",
       "499 -0.707040 -0.424800 -0.707040  8.373279 -3.600000 -3.636000\n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(agent.q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显示最优策略估计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1    2    3    4    5\n",
       "0    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "1    0.0  0.0  0.0  0.0  1.0  0.0\n",
       "2    0.0  0.0  0.0  0.0  1.0  0.0\n",
       "3    0.0  0.0  0.0  0.0  1.0  0.0\n",
       "4    1.0  0.0  0.0  0.0  0.0  0.0\n",
       "..   ...  ...  ...  ...  ...  ...\n",
       "495  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "496  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "497  0.0  1.0  0.0  0.0  0.0  0.0\n",
       "498  1.0  0.0  0.0  0.0  0.0  0.0\n",
       "499  0.0  0.0  0.0  1.0  0.0  0.0\n",
       "\n",
       "[500 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = np.eye(agent.action_n)[agent.q.argmax(axis=-1)] \n",
    "pd.DataFrame(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 期望 SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSAAgent:\n",
    "    def __init__(self, env, gamma=0.9, learning_rate=0.1, epsilon=.01):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.action_n = env.action_space.n\n",
    "        \n",
    "    def decide(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            action = self.q[state].argmax()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        v = (self.q[next_state].mean() * self.epsilon + \\\n",
    "                self.q[next_state].max() * (1. - self.epsilon))\n",
    "        u = reward + self.gamma * v * (1. - done)\n",
    "        td_error = u - self.q[state, action]\n",
    "        self.q[state, action] += self.learning_rate * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_qlearning(env, agent, train=False, render=False):\n",
    "    episode_reward = 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = agent.decide(observation)\n",
    "        next_observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        if train:\n",
    "            agent.learn(observation, action, reward, next_observation,\n",
    "                    done)\n",
    "        if done:\n",
    "            break\n",
    "        observation = next_observation\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ExpectedSARSAAgent(env)\n",
    "\n",
    "# 训练\n",
    "episodes = 5000\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_qlearning(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# 测试\n",
    "agent.epsilon = 0. # 取消探索\n",
    "\n",
    "episode_rewards = [play_qlearning(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),\n",
    "        len(episode_rewards), np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, gamma=0.9, learning_rate=0.1, epsilon=.01):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.action_n = env.action_space.n\n",
    "        self.q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        \n",
    "    def decide(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            action = self.q[state].argmax()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        u = reward + self.gamma * self.q[next_state].max() * (1. - done)\n",
    "        td_error = u - self.q[state, action]\n",
    "        self.q[state, action] += self.learning_rate * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = QLearningAgent(env)\n",
    "\n",
    "# 训练\n",
    "episodes = 4000\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_qlearning(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# 测试\n",
    "agent.epsilon = 0. # 取消探索\n",
    "\n",
    "episode_rewards = [play_qlearning(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),\n",
    "        len(episode_rewards), np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 双重 Q 学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, gamma=0.9, learning_rate=0.1, epsilon=.01):\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.action_n = env.action_space.n\n",
    "        self.q0 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        self.q1 = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        \n",
    "    def decide(self, state):\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            action = (self.q0 + self.q1)[state].argmax()\n",
    "        else:\n",
    "            action = np.random.randint(self.action_n)\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        if np.random.randint(2):\n",
    "            self.q0, self.q1 = self.q1, self.q0\n",
    "        a = self.q0[next_state].argmax()\n",
    "        u = reward + self.gamma * self.q1[next_state, a] * (1. - done)\n",
    "        td_error = u - self.q0[state, action]\n",
    "        self.q0[state, action] += self.learning_rate * td_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleQLearningAgent(env)\n",
    "\n",
    "# 训练\n",
    "episodes = 9000\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_qlearning(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# 测试\n",
    "agent.epsilon = 0. # 取消探索\n",
    "\n",
    "episode_rewards = [play_qlearning(env, agent) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),\n",
    "        len(episode_rewards), np.mean(episode_rewards)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA($\\lambda $) 算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSALambdaAgent(SARSAAgent):\n",
    "    def __init__(self, env, lambd=0.6, beta=1.,\n",
    "            gamma=0.9, learning_rate=0.1, epsilon=.01):\n",
    "        super().__init__(env, gamma=gamma, learning_rate=learning_rate,\n",
    "                epsilon=epsilon)\n",
    "        self.lambd = lambd\n",
    "        self.beta = beta\n",
    "        self.e = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        \n",
    "    def learn(self, state, action, reward, next_state, done, next_action):\n",
    "        # 更新资格迹\n",
    "        self.e *= (self.lambd * self.gamma)\n",
    "        self.e[state, action] = 1. + self.beta * self.e[state, action]\n",
    "\n",
    "        # 更新价值\n",
    "        u = reward + self.gamma * \\\n",
    "                self.q[next_state, next_action] * (1. - done)\n",
    "        td_error = u - self.q[state, action]\n",
    "        self.q += self.learning_rate * self.e * td_error\n",
    "        if done:\n",
    "            self.e *= 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SARSALambdaAgent(env)\n",
    "\n",
    "# 训练\n",
    "episodes = 5000\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    episode_reward = play_sarsa(env, agent, train=True)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    \n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "# 测试\n",
    "agent.epsilon = 0. # 取消探索\n",
    "\n",
    "episode_rewards = [play_sarsa(env, agent, train=False) for _ in range(100)]\n",
    "print('平均回合奖励 = {} / {} = {}'.format(sum(episode_rewards),\n",
    "        len(episode_rewards), np.mean(episode_rewards)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
